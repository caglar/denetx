using no cache
Reading from train-sets/0002.dat
num sources = 1
Num weight bits = 18
learning rate = 10
initial_t = 1
power_t = 0.5
predictions = 0002b.predict
only testing
average    since       example  example    current  current  current
loss       last        counter   weight      label  predict features
0.005051   0.005051          3      3.0     0.5498   0.4934       15
0.024870   0.044689          6      6.0     0.2681   0.5750       15
0.029170   0.034329         11     11.0     0.4315   0.5579       15
0.038323   0.047476         22     22.0     0.5519   0.4414       15
0.031144   0.023966         44     44.0     0.5514   0.4398       15
0.025148   0.019013         87     87.0     0.5140   0.4770       15
0.021792   0.018435        174    174.0     0.5596   0.4304       15
0.017411   0.013030        348    348.0     0.5475   0.4530       15
0.013651   0.009892        696    696.0     0.3421   0.5639       15
0.013462   0.013273       1392   1392.0     0.4996   0.5188       15
0.009169   0.004877       2784   2784.0     0.5090   0.4615       15
0.006852   0.004535       5568   5568.0     0.6413   0.4012       15
0.006922   0.006991      11135  11135.0     0.3869   0.4850       15
0.008017   0.009112      22269  22269.0     0.5063   0.4603       15
0.008291   0.008564      44537  44537.0     0.4905   0.4934       15

finished run
number of examples = 74746
weighted example sum = 6.952e+04
weighted label sum = 3.511e+04
average loss = 0.007294
best constant = 0.5051
best constant's loss = 0.25
total feature number = 1121190
